\section{Related Work}
\label{sec:related_work}

\paragraph{Weight-based watermarking in open-source models.}

Recent work explores different ways of modifying model weights to embed watermarks. One approach is watermark distillation \citep{gu2023learnability}, where a student model is trained to reproduce outputs from a decoding-time-watermarked teacher model. While this strategy can transfer the watermark, it is sensitive to post-hoc fine-tuning, and learning low-distortion watermarks requires a lot of training data. Another approach uses reinforcement learning to embed watermarks \citep{xu2024learningwatermarkllmgeneratedtext}. In this work, a detector model, trained jointly with the LLM, acts as a reward model in the reinforcement learning objective, steering the LLM to produce watermarked text while preserving fluency and utility. While this approach offers some benefits over distillation, it requires substantial computational resources and involves complex policy-based training objectives. \citet{elhassan2025can} eliminates the need for explicit reward modeling by directly incorporating a fully-differentiable detection objective into the training loss and uses low-rank adaptation \citet{hu2022lora} for efficient training. Despite its benefits, the approach relies on a min-max optimization that can be difficult to train stably.

Other approaches modify weights without retraining. \citet{christ2024provably} introduce a method that adds a Gaussian perturbation to the final-layer bias vector, inducing subtle but consistent shifts in token logits. These biases accumulate across generated tokens, enabling watermark detection. However, the approach relies on final-layer biases—which many LLMs omit—and the watermark can be easily erased by removing the bias. \citet{block2025gaussmark} propose injecting Gaussian perturbations into decoder weights, with detection based on computing the dot product between the noise vector and the gradient of the log-likelihood. This method requires both forward and backward passes and is highly sensitive to noise placement and magnitude.