\section{Introduction}

As open-source large language models (LLMs) become increasingly capable and widely available, ensuring reliable attribution of AI-generated content has become more urgent
\sr{not clear why capable and widely available models require reliable attribution,
add a line of why due to issues like xxx we need reliable attribution}
. Watermarking—embedding imperceptible statistical signals into generated text—
\sr{do not need the erm dash here.}
has emerged as a promising technique to trace provenance and deter misuse. Traditional watermarking methods, however, rely on generation-time interventions that modify the decoding process to guide token selection \cite{kirchenbauer2023watermark}. These techniques are fundamentally incompatible with open-weight settings, where end-users have full access to and control over the model internals, including the sampling strategy.

To address this limitation, recent work embeds watermarks directly into a model's weights so it naturally generates watermarked text. Methods include distillation, which fine-tunes the model on outputs from a decoder-based watermarked teacher \cite{gu2023learnability}; reinforcement learning, using a separate detector model to assign rewards that guide watermark insertion \cite{xu2024learningwatermarkllmgeneratedtext}; and LoRA-based fine-tuning \cite{hu2022lora}, in which lightweight modules and a detector are trained jointly to encourage both watermark generation and detection \cite{elhassan2025can}. While effective, these approaches are often computationally expensive.

Edit-based watermarking methods 
\sr{we need to define this term before using it}
offer a lightweight alternative by directly modifying select model weights without any retraining. For instance, \textsc{Provably Robust} \cite{christ2024provably} introduces a Gaussian watermark into a newly added bias vector in the output layer. 
\sr{not sure if this a component added, since its there but just set to zero, let's focus on the that it simulates fixed list - not safe and easier to remove?}
While effective, this added component is non-standard and can be easily stripped without affecting model behavior. \textsc{Gaussmark} \cite{block2025gaussmark}, in contrast, avoids architectural changes by perturbing existing weight subsets and detects watermarks using a z-score computed from the dot product between the perturbation and the gradient of log-likelihood. However, a limitation of \textsc{Gaussmark} is that the detection signal is weak and the method requires both a forward pass and a partial backward pass, making it less practical for deployment.

In this work, we introduce a new
\sr{i would avoid claiming new}
\emph{edit-based watermarking framework} that modifies the unembedding layer weights—the parameters that project final hidden states to output vocabulary logits—by adding structured perturbations. These perturbations induce dynamic logit biases during generation that subtly influence token sampling in a detectable way. Unlike prior logit-based watermarking strategies such as \citet{kirchenbauer2023watermark} and \citet{liu2024adaptive}, which inject watermarks during decoding, our approach directly embeds the biasing logic into the model's weights. This design ensures that the watermark cannot be easily removed or circumvented, even when users have full control over the decoding process or inference code. Moreover, our framework is general-purpose: it supports multiple watermarking instantiations that produce different biasing patterns, all of which are compatible with a unified detection method.

To detect the watermark, we apply a simple and scalable test: the \textbf{average log-likelihood ratio (LLR)} per token between the watermarked model and a reference model.
\sr{we should unwatermarked model unless the reference model can be something entirely different}
This test captures subtle but consistent shifts in token probabilities introduced by the watermark. Crucially, it only requires forward passes and works across all watermarking strategies instantiated under our framework, provided they produce detectable logit perturbations.

We evaluate our method on three popular open-source LLMs, Llama-2-7b, Mistral-7B-v0.3 and Qwen2.5-3B under a range of conditions. Our results demonstrate that the proposed approach consistently achieves:

\begin{enumerate}

    \item Strong watermark detectability using the average LLR test

    \item Minimal impact on generation quality

    \item Robustness to paraphrasing and fine-tuning.

\end{enumerate}

The rest of the paper is organized as follows: Section~\ref{sec:background_related} provides background on language model watermarking, with a focus on the unique challenges posed by open-source settings. Section~\ref{sec:methodology} introduces a general unembedding-layer watermarking framework, along with two concrete instantiations and a corresponding statistical detection method. Section~\ref{sec:experiments} presents empirical evaluations of watermark detectability and text quality, including downstream task performance. Section~\ref{sec:paraphasing} examines robustness to paraphrasing attacks, and Section~\ref{sec:finetuning} investigates the durability of the watermark under model fine-tuning. Section~\ref{sec:conclusion} summarizes our findings and outlines directions for future work.