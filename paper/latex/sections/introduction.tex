\section{Introduction}

Watermarking techniques aim to embed imperceptible signals into model-generated text to enable provenance detection and mitigate misuse. While most existing approaches operate at decoding time by modifying next-token probabilities \citep{kirchenbauer2023watermark, aaronson2023reform, kuditipudi2023robust, liu2024adaptive}, these methods can be easily bypassed in open-weight models, where users have full control over the generation process. This motivates a shift toward techniques that embed the watermarking logic directly into the model's weights to produce detectable artifacts in the output distribution.

Several existing approaches for generating watermarked text rely on fine-tuning language models \cite{gu2023learnability,xu2024learningwatermarkllmgeneratedtext,elhassan2025can}. These tuning-based methods are often computationally intensive, require complex training pipelines. Other techniques embed watermarking logic by directly modifying model weights \cite{christ2024provably, block2025gaussmark}. However, such modifications can be easily removed without degrading model performance, or they produce weak detection signals, limiting their practical applicability.

In this work, we introduce a new \emph{watermarking framework} that modifies the unembedding layer weights—the parameters that project final hidden states to output vocabulary logits—by adding structured perturbations. These perturbations induce dynamic logit biases during generation that subtly influence token sampling in a detectable way. Unlike prior logit-based watermarking strategies such as \citet{kirchenbauer2023watermark} and \citet{liu2024adaptive}, which bias logit values during decoding, our approach directly embeds the biasing logic into the unembedding layer's weights. This design ensures that the watermark cannot be easily removed or circumvented, even when users have full control over the decoding process or inference code. Moreover, our framework is general-purpose: it supports multiple watermarking schemes that produce different biasing patterns, all of which are compatible with a unified detection method.

To detect the watermark, we use a simple classification rule: the \textbf{average log-likelihood ratio (LLR)} per token between the watermarked model and the base model. This rule captures subtle but consistent shifts in token probabilities introduced by the watermark. Crucially, it only requires forward passes and works across multiple watermarking schemes within our framework, provided they produce detectable logit perturbations.

We evaluate our method on three widely used open-source LLMs—LLaMA-2-7B \citep{touvron2023llama}, Mistral-7B-v0.3 \citep{jiang2023mistral7b}, and Qwen2.5-3B \citep{qwen2025qwen25technicalreport}—under a variety of conditions. Our results show that the proposed approach consistently achieves high detection accuracy across models and data distributions, while preserving text quality as measured by perplexity and performance on downstream tasks \todo{Add numbers here}. Additionally, we demonstrate that our method is reasonably robust to paraphrasing attacks and remains effective after post-hoc fine-tuning, highlighting its practicality for watermarking open-weight models.