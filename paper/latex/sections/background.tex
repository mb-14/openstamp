\section{Background}
\label{sec:background}

\subsection{Watermarking in Language Models}

Watermarking techniques aim to embed subtle signals into model-generated text that are imperceptible to humans but detectable through statistical analysis. Most existing approaches operate at decoding time, modifying next-token probabilities based on pseudorandom functions that assign scores or preferences to tokens in the vocabulary. A common strategy is to define a favored subset of tokens—the green list—and then bias generation toward these tokens using additive logit shifts or sampling-based constraints. For example, \citet{kirchenbauer2023watermark} apply soft logit biasing toward green-listed tokens, while \citet{aaronson2023reform} and \citet{kuditipudi2023robust} employ cryptographically driven scores to guide token sampling. Detection is typically performed through statistical tests that evaluate whether the distribution of generated tokens aligns with the expected green list patterns.

These methods typically involve a trade-off between two competing objectives: text generation quality and detection strength. Strong watermarking configurations—such as those using aggressive logit biasing or restrictive sampling—are easier to detect but tend to degrade text quality. In contrast, low-distortion watermarking setups preserve output quality but often result in signals that are harder to detect reliably.

\subsection{Open-Source Model Challenges}

\paragraph{Limitations of decoding-time watermarking.}
In open-source settings, where users have full access to and control over the model, decoding-time watermarking becomes ineffective since the decoding logic can be easily bypassed. This has led to increasing interest in weight-based watermarking strategies that embed the watermarking process directly into the model's parameters. We discuss these methods in detail in Section~\ref{sec:related_work}.

\paragraph{Durability to Model Modifications.}
Durability is a central challenge in watermarking open-source models, which are frequently modified through quantization, pruning, merging, or fine-tuning. \citet{gloaguen2025towards} systematically evaluate existing approaches and find that none remain consistently detectable under such modifications.  In particular, distillation-based methods suffer from watermark decay under light supervision—even a few hundred steps of fine-tuning can erase the signal. Weight-editing schemes, while training-free, are often vulnerable to parameter shifts introduced by quantization or model merging. Their findings highlight the need for watermarking techniques designed explicitly with durability in mind.

\paragraph{Impact on downstream performance.}
Text quality is not the only metric affected by watermarking—\citet{ajith-etal-2024-downstream} show that even moderate-strength watermarks can significantly degrade performance on downstream tasks such as classification, QA, and generation. Since open-source watermarking methods embed the signal directly into model weights and cannot be easily undone once released, it is especially important to ensure that downstream task performance remains unaffected.