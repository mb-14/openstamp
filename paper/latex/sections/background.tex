\section{Background and Related Work}
\label{sec:background_related}

\subsection{Watermarking in Language Models}

Watermarking techniques aim to embed subtle signals into model-generated text that are imperceptible to humans but detectable through statistical analysis. Most existing approaches operate at decoding time, modifying next-token probabilities based on pseudorandom functions that assign scores or preferences to tokens in the vocabulary. A common strategy is to define a favored subset of tokens—the green list—by hashing the previous \(k\) tokens, and then bias generation toward these tokens using additive logit shifts or sampling-based constraints. For example, \citet{kirchenbauer2023watermark} apply soft logit biasing toward green-listed tokens, while \citet{aaronson2023reform} and \citet{kuditipudi2023robust} employ cryptographically driven scores to guide token sampling. Detection is typically performed through statistical tests that evaluate whether the distribution of generated tokens aligns with the expected green list patterns.

These methods typically involve a trade-off between two competing objectives: text generation quality and detection strength. Strong watermarking configurations—such as those using aggressive logit biasing or restrictive sampling—are easier to detect but tend to degrade text quality. In contrast, low-distortion watermarking setups preserve output quality but often result in signals that are harder to detect reliably.

\subsection{Open-Source Model Challenges}

In open-source settings, where users have full access to and control over the model, decoding-time watermarking becomes ineffective since the decoding logic can be easily bypassed. This has led to increasing interest in weight-based watermarking strategies that embed signals directly into the model's parameters.

Recent work explores different ways of modifying model weights to embed watermarks. One approach is watermark distillation \citep{gu2023learnability}, where a student model is trained to reproduce outputs from a decoding-time-watermarked teacher model. While this strategy can transfer the watermark, it is sensitive to post-hoc fine-tuning, and learning low-distortion watermarks requires a lot of training data. Another approach uses reinforcement learning to embed watermarks \citep{xu2024learningwatermarkllmgeneratedtext}. In this work, a detector model, trained jointly with the LLM, acts as a reward model in the reinforcement learning objective, steering the LLM to produce watermarked text while preserving fluency and utility. While this approach offers some benefits over distillation, it requires substantial computational resources and involves complex policy-based training objectives. \citet{elhassan2025can} eliminates the need for explicit reward modeling by directly incorporating a fully-differentiable detection objective into the training loss and uses low-rank adaptation \citet{hu2022lora} for efficient training. Despite its benefits, the approach relies on a min-max optimization that can be difficult to train stably.

Other approaches modify weights without retraining. \citet{christ2024provably} introduce a method that adds a Gaussian perturbation to the final-layer bias vector, inducing subtle but consistent shifts in token logits. These biases accumulate across generated tokens, enabling watermark detection. However, the approach relies on final-layer biases—which many LLMs omit—and the watermark can be easily erased by removing the bias. \citet{block2025gaussmark} propose injecting Gaussian perturbations into decoder weights, with detection based on computing the dot product between the noise vector and the gradient of the log-likelihood. This method requires both forward and backward passes and is highly sensitive to noise placement and magnitude.

\paragraph{Durability to Model Modifications.}
Durability is a central challenge in watermarking open-source models, which are frequently modified through quantization, pruning, merging, or fine-tuning. \citet{gloaguen2025towards} systematically evaluate existing approaches and find that none remain consistently detectable under such modifications.  In particular, distillation-based methods suffer from watermark decay under light supervision—even a few hundred steps of fine-tuning can erase the signal. Weight-editing schemes, while training-free, are often vulnerable to parameter shifts introduced by quantization or model merging. Their findings highlight the need for watermarking techniques designed explicitly with durability in mind.

\paragraph{Impact on downstream performance.}
Text quality is not the only metric affected by watermarking—\citet{ajith-etal-2024-downstream} show that even moderate-strength watermarks can significantly degrade performance on downstream tasks such as classification, QA, and generation. Since open-source watermarking methods embed the signal directly into model weights and cannot be easily undone once released, it is especially important to ensure that downstream task performance remains unaffected.