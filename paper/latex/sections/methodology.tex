\section{Methodology}
\label{sec:methodology}

This section presents our framework for watermarking language model outputs. We begin by formalizing the model setup and notation. We then describe the general mechanism for inducing logit-level watermark signals through linear operations on hidden states and specify the desired properties of such perturbations. Finally, we provide two illustrative instantiations that demonstrate how this framework can be instantiated in practice.

\subsection{Preliminaries}

Let a causal language model process a sequence of tokens drawn from a vocabulary \( \mathcal{V} = \{w_1, w_2, \dots, w_{|\mathcal{V}|}\} \). Let \( x_i \in \mathcal{V} \) denote the \( i \)-th token in the sequence, and let \( x_{<t} = (x_1, \dots, x_{t-1}) \) denote the prefix up to timestep \( t \).

The model computes a hidden representation \( h_t = f(x_{<t}) \in \mathbb{R}^d \), where \( f: \mathcal{V}^* \rightarrow \mathbb{R}^d \) is the model's internal encoding function. The logit vector \( v_t \in \mathbb{R}^{|\mathcal{V}|} \) for predicting the next token is given by:
\begin{equation}
    v_t = U h_t,
\end{equation}
where \( U \in \mathbb{R}^{|\mathcal{V}| \times d} \) is the unembedding matrix.

\subsection{A Framework for Watermarking via Unembedding Perturbation}

We present a general framework for watermarking the outputs of language models by introducing hidden-state-dependent biases to the token logits by adding a structured perturbation to \( U \). Specifically, we define a perturbed unembedding matrix:
\begin{equation}
    \tilde{U} = U + \Delta W,
\end{equation}
where \( \Delta W \in \mathbb{R}^{|\mathcal{V}| \times d} \) is the perturbation matrix. This results in a modified logit vector at timestep \( t \):
\begin{equation}
    \tilde{v}_t = \tilde{U} h_t = v_t + \Delta W h_t,
\end{equation}
The perturbation introduces an additive logit bias that varies with the internal state of the model and serves as a watermarking signal.

To be effective, the perturbation \( \Delta W \) should satisfy the following requirements:

\begin{itemize}
    \item \textbf{Detectable:} The induced logit biases should cause the generated text to accumulate identifiable signals that distinguish watermarked outputs from typical model generations. These signals should be reliably detectable using statistical or non-statistical methods applied to the generated text.


    \item \textbf{Controllable:} The perturbation should be parameterized to support explicit control over (i) the detectability or strength of the watermark signal, and (ii) the impact on text quality metrics such as fluency and perplexity. This enables a tunable tradeoff between visibility and stealth.

    \item \textbf{Logit variability:} The perturbation should induce logit biases that vary across timesteps and inputs. Fixed or invariant biases—such as those used in fixed green list schemes—are vulnerable to reverse engineering, as shown by \citet{rastogi2024revisitingrobustness}. In our framework, variability arises naturally since the logit perturbation is computed as a linear operation on the hidden state, which itself varies with context.

\end{itemize}

\paragraph{Norm-Based Analysis of Perturbation Strength.}
The total influence of \( \Delta W \) on the model's output logits can be analyzed using its Frobenius norm:
\[
    \|\Delta W\|_F = \left( \sum_{i=1}^{|\mathcal{V}|} \sum_{j=1}^{d} (\Delta W_{ij})^2 \right)^{1/2}.
\]
This norm provides a global upper bound on the magnitude of the logit perturbation at each timestep:
\[
    \|\Delta W h_t\| \leq \|\Delta W\|_F \cdot \|h_t\|.
\]

In practice, the norm \( \|h_t\| \) is typically stable and predictable in pretrained language models due to the widespread use of layer normalization \citep{ba2016layer}, which standardizes hidden activations across layers. As a result, we can approximate the bound using a model-dependent constant \( \mu_h \), the expected hidden state norm:
\[
    \|\Delta W h_t\| \lessapprox \|\Delta W\|_F \cdot \mu_h.
\]
While \( \mu_h \) may vary across architectures and layers, it is generally consistent within a given model.

The Frobenius norm serves as a useful and interpretable analytic quantity for characterizing the potential effect on model behavior. In contrast, \citet{block2025gaussmark} applies perturbations to arbitrary subsets of model weights, making their effect on output logits indirect and difficult to predict. As a result, tuning for detectability and output quality relies heavily on empirical calibration. In our framework, the perturbation is applied directly at the logit level, and its impact can be more deterministically understood by analyzing the norm of \( \Delta W \). Moreover, \( \|\Delta W\|_F \) provides a meaningful way to constrain the overall strength of the perturbation during the design or parameterization of \( \Delta W \).


\subsection{Illustrative Examples}
This framework accommodates a broad family of parameterizations for \( \Delta W \) that satisfy the above criteria. We now describe two example instantiations that demonstrate how the framework can be applied in practice.

\subsubsection{Green List Biasing}

This method draws inspiration from the watermarking approach of \citet{kirchenbauer2023watermark}, which boosts the logits of tokens belonging to a pseudorandomly selected \emph{green list}. At each timestep, the green list is generated using a pseudorandom function (PRF) that depends on a secret seed and the preceding \( n \) tokens in the prefix. This ensures that the token biases vary in a deterministic, structured, and context-sensitive manner.

To encode this behavior directly into the model weights, we express the perturbation matrix \( \Delta W \) as the product of two matrices:
\begin{equation}
    \Delta W = G H,
\end{equation}
where:

\begin{itemize}
    \item \( G \in \mathbb{R}^{|\mathcal{V}| \times C} \) contains \( C \) watermarking lists represented as row vectors. For each pseudo-class \( c \in \{1, \dots, C\} \), the corresponding row \( H_c \in \mathbb{R}^{|\mathcal{V}|} \) is defined as:
          \[
              (H_c)_i =
              \begin{cases}
                  \delta & \text{if } i \in \mathcal{G}_c \\
                  0      & \text{otherwise}
              \end{cases}
          \]
          where \( \delta > 0 \) is a fixed scalar that determines the strength of the watermark signal, and \( \mathcal{G}_c \subseteq \{1, \dots, |\mathcal{V}|\} \) is the green list for class \( c \), constructed using a pseudorandom function:
          \[
              \mathcal{G}_c = \left\{ i \in \{1, \dots, |\mathcal{V}|\} \;\middle|\; \mathrm{PRF}(\text{seed}, c, i) < \gamma \right\}.
          \]
          Here, \(\mathrm{PRF}(\cdot)\) is a keyed hash function seeded with a secret key, and \(\gamma \in (0,1)\) is a fixed threshold that determines the fraction of green-listed indices.

    \item \( H \in \mathbb{R}^{C \times d} \) maps each token's hidden state to a soft pseudo-class one-hot selector. To construct \( H \), we proceed as follows:
          \begin{enumerate}
              \item Run the base model on a large unlabeled corpus (e.g., OpenWebText) and collect hidden states from the last layer before the unembedding layer.
              \item Apply \( k \)-means clustering to these hidden states, assigning each to one of \( C \) pseudo-classes.
              \item Solve a ridge regression problem to map each hidden state \( h \in \mathbb{R}^d \) to a soft one-hot selector \( s \in \mathbb{R}^C \), approximating the discrete cluster assignments.
          \end{enumerate}
\end{itemize}

This construction ensures that hidden states in similar regions of representation space receive similar perturbations. Although the selectors in \( G \) are soft, they closely approximate discrete assignments and enable reliable watermark detection. The structured factorization of \( \Delta W \) into \( G \) and \( H \) allows multiple green list rules to be encoded in a single matrix perturbation.

\subsubsection{Gaussian Random Projection}

In this variant, the watermark is embedded by applying a fixed Gaussian perturbation to the unembedding matrix. Specifically, the perturbation matrix \( \Delta W \in \mathbb{R}^{|\mathcal{V}| \times d} \) is initialized as:
\begin{equation}
    \Delta W_{ij} \sim \mathcal{N}(0, 1).
\end{equation}

During generation, this produces a dynamic, input-dependent logit bias:
\[
    \Delta \ell_t = \Delta W h_t \in \mathbb{R}^{|\mathcal{V}|},
\]
which is added to the model’s original logits.

Unlike the green list biasing approach, which relies on discrete vocabulary partitions, this method introduces a continuous watermarking signal by projecting the hidden state through a random Gaussian matrix. Although the hidden state \( h_t \in \mathbb{R}^d \) is not necessarily Gaussian, the Central Limit Theorem ensures that the projected logits \( [\Delta \ell_t]_i = \langle \Delta W_i, h_t \rangle \) are approximately Gaussian-distributed when \( d \) is large:

\begin{theorem}[CLT for Logit Bias Projection]
    Let \( h_t \in \mathbb{R}^d \) be fixed, and let each row \( \Delta W_i \in \mathbb{R}^d \) be drawn independently from \( \mathcal{N}(0, I_d) \). Then the scalar projection
    \[
        [\Delta W h_t]_i = \langle \Delta W_i, h_t \rangle = \sum_{j=1}^d \Delta W_{ij} \cdot h_{tj}
    \]
    converges in distribution to \( \mathcal{N}(0, \|h_t\|^2) \) as \( d \to \infty \).
\end{theorem}

Consequently, the logit bias vector \( \Delta \ell_t \) is approximately distributed as \( \mathcal{N}(0, \|h_t\|^2 I) \), meaning that, at each timestep, about half of the tokens are boosted while the other half are suppressed. This introduces a subtle but statistically detectable watermark signal without degrading generation quality.

To ensure consistent perturbation magnitude across timesteps and model scales, we normalize the matrix using the expected norm of a hidden state vector and introduce a scaling hyperparameter \( \delta > 0 \) to control watermark strength:
\begin{equation}
    \Delta W \leftarrow \frac{\delta}{\mathbb{E}[\|h_t\|]} \cdot \Delta W.
\end{equation}

\subsection{Detection via Likelihood Ratio Test}

To detect the watermark, we use a length-normalized log-likelihood ratio (LLR) test between the watermarked and reference models:
\begin{equation} \label{eq:llr}
    \text{LLR}(x) = \frac{1}{T} \sum_{t=1}^{T} \log \frac{p_{\text{wm}}(x_t \mid x_{<t})}{p_{\text{ref}}(x_t \mid x_{<t})},
\end{equation}
where \( p_{\text{wm}} \) and \( p_{\text{ref}} \) denote the softmax probabilities computed using the watermarked and original unembedding matrices, respectively.

These probabilities are defined as:
\begin{align}
    p_{\text{ref}}(x_t \mid x_{<t}) & =
    \frac{\exp(U_{x_t} h_t)}{\sum_{j=1}^{|V|} \exp(U_j h_t)}, \\
    p_{\text{wm}}(x_t \mid x_{<t})  & =
    \frac{\exp(\tilde{U}_{x_t} h_t)}{\sum_{j=1}^{|V|} \exp(\tilde{U}_j h_t)},
\end{align}
where \( U \in \mathbb{R}^{|V| \times d} \) is the original unembedding matrix, \( \tilde{U} = U + \Delta W \) is the watermarked version, and \( h_t \in \mathbb{R}^d \) is the hidden state at timestep \( t \).

Substituting these into Equation~\ref{eq:llr}, we obtain:
\begin{equation}
    \begin{aligned}
        \text{LLR}(x)
         & = \frac{1}{T} \sum_{t=1}^{T} \Big(
        (\tilde{U}_{x_t} - U_{x_t}) h_t                                       \\
         & \quad - \log \frac{\sum_j e^{\tilde{U}_j h_t}}{\sum_j e^{U_j h_t}}
        \Big)
    \end{aligned}
\end{equation}

This expression decomposes the LLR into two interpretable terms:
\begin{itemize}
    \item A \textit{token-level logit shift} term, \( (\tilde{U}_{x_t} - U_{x_t}) h_t \), which directly reflects the effect of watermarking on the predicted token's logit.
    \item A \textit{partition function ratio} term, which captures the normalization difference across the entire vocabulary.
\end{itemize}

\paragraph{Detection Protocol.}
The model developer releases the model with the watermarked unembedding matrix \( \tilde{U} = U + \Delta W \), while keeping the original matrix \( U \)—and by extension the perturbation \( \Delta W \)—secret. To verify whether a given piece of text \( x \) was generated by the watermarked model, the developer computes the LLR between the public (watermarked) model and the hidden reference model using Equation~\ref{eq:llr}. A significantly positive LLR indicates the presence of the watermark signal.

This detection process can be kept private or deployed via a walled API, allowing third parties to query the watermark status of text without exposing the reference model or the perturbation matrix.


% TODO:
% - Describe expected value of LLR under the null hypothesis (no watermark) and the alternative hypothesis (watermark present).
% - Discuss why a LLR is better than logit difference. Show rigorousness justification.