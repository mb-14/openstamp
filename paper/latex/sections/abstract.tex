\begin{abstract}
With the rise of human-like text generation by large language models (LLMs), reliably attributing machine-generated content has become increasingly important for combating misinformation, ensuring content provenance, and enforcing responsible AI usage. Watermarking, which embeds identifiable statistical signals in the generated text, offers a promising solution. However, existing watermarking methods typically alter the sampling process during generation, making them vulnerable in open-weight settings where users have unrestricted control over how text is produced. Existing open-weight watermarking methods either require costly tuning strategies, exhibit weak detection strength, or can be trivially removed without affecting model performance. In this work, we propose a method that directly modifies the unembedding layer through a structured perturbation conditioned on the model's hidden states, in order to steer generation toward watermarked outputs. Our approach is training-free and integrates much faster than tuning-based watermarking methods. We conduct experiments demonstrating improved watermark detectability compared to existing approaches, without degrading text quality. To further assess practical utility, we evaluate the method's robustness to paraphrasing attacks, durability under post-hoc finetuning, and performance across a range of downstream tasks.
\end{abstract}P
