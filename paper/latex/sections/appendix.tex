\appendix
\section{Norm-Based Analysis of Perturbation Strength}
\label{app:norm_analysis}

The total influence of \( \Delta W \) on the model's output logits can be analyzed using its Frobenius norm:
\[
    \|\Delta W\|_F = \left( \sum_{i=1}^{|\mathcal{V}|} \sum_{j=1}^{d} (\Delta W_{ij})^2 \right)^{1/2}.
\]
This norm provides a global upper bound on the magnitude of the logit perturbation at each timestep:
\[
    \|\Delta W h_t\| \leq \|\Delta W\|_F \cdot \|h_t\|.
\]

In practice, the norm \( \|h_t\| \) is typically stable and predictable in pretrained language models due to the widespread use of layer normalization \citep{ba2016layer}, which standardizes hidden activations across layers. As a result, we can approximate the bound using a model-dependent constant \( \mu_h \), the expected hidden state norm:
\[
    \|\Delta W h_t\| \lessapprox \|\Delta W\|_F \cdot \mu_h.
\]
While \( \mu_h \) may vary across architectures and layers, it is generally consistent within a given model.

The Frobenius norm serves as a useful and interpretable analytic quantity for characterizing the potential effect on model behavior. In contrast, \citet{block2025gaussmark} applies perturbations to arbitrary subsets of model weights, making their effect on output logits indirect and difficult to predict. As a result, tuning for detectability and output quality relies heavily on empirical calibration. In our framework, the perturbation is applied directly at the logit level, and its impact can be more deterministically understood by analyzing the norm of \( \Delta W \). Moreover, \( \|\Delta W\|_F \) provides a meaningful way to constrain the overall strength of the perturbation during the design or parameterization of \( \Delta W \).
