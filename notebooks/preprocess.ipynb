{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7251e35f69f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rich import print as rprint\n",
    "from typing import Dict, List\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"MODEL\", \"meta-llama/Llama-2-7b-hf\")\n",
    "total = int(os.getenv(\"TOTAL\", 1000))\n",
    "dataset_name = \"Skylion007/openwebtext\"\n",
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "dataset_suffix = dataset_name.split(\"/\")[-1]\n",
    "balance_interval = 25\n",
    "batch_size = 128\n",
    "max_seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(example):\n",
    "    return len(tokenizer(example[\"text\"])['input_ids']) >= max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name,\n",
    "                       split=\"train\", streaming=True, trust_remote_code=True)\n",
    "dataset = dataset.filter(filter_length)\n",
    "\n",
    "dataset = dataset.shuffle(seed=42).take(total * batch_size)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(prefixes):\n",
    "    prefixes_vec = torch.cat(prefixes, dim=0)\n",
    "    # Find the last non-padding token for each sequence\n",
    "    non_pad_mask = prefixes_vec != tokenizer.pad_token_id\n",
    "    last_token_indices = non_pad_mask.sum(dim=1) - 1\n",
    "    batch_size = prefixes_vec.size(0)\n",
    "    token_ids_vec = prefixes_vec[torch.arange(batch_size), last_token_indices]\n",
    "\n",
    "    S = len(token_ids_vec)\n",
    "    unique_classes, class_counts = torch.unique(\n",
    "        token_ids_vec, return_counts=True)\n",
    "    num_classes = len(unique_classes)\n",
    "    samples_per_class = max(1, S // num_classes)\n",
    "\n",
    "    selected_indices = []\n",
    "    for cls, count in zip(unique_classes, class_counts):\n",
    "        class_indices = (token_ids_vec == cls).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Sample min(samples_per_class, available samples)\n",
    "        sampled_indices = class_indices[torch.randperm(\n",
    "            count)[:min(samples_per_class, count)]]\n",
    "        selected_indices.append(sampled_indices)\n",
    "\n",
    "    # Concatenate once instead of multiple times\n",
    "    selected_indices = torch.cat(selected_indices)\n",
    "\n",
    "    # Shuffle to ensure randomness\n",
    "    shuffled_indices = selected_indices[torch.randperm(len(selected_indices))[\n",
    "        :S]]\n",
    "\n",
    "    return [prefixes_vec[shuffled_indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index = 0\n",
    "all_prefixes = []\n",
    "\n",
    "for batch in dataloader:\n",
    "    text = batch[\"text\"]\n",
    "    input_ids = tokenizer(\n",
    "        text, truncation=True, max_length=max_seq_len, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Convert input_ids [batch_size, seq_len] to a list of prefixes of varying lengths from 1 to seq_len\n",
    "    prefixes = [input_ids[:, :i + 1].T for i in range(input_ids.shape[1])]\n",
    "    \n",
    "    # Add padding to the prefixes\n",
    "    padded_prefixes = torch.nn.utils.rnn.pad_sequence(\n",
    "        prefixes, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Reshape to [batch_size, seq_len]\n",
    "    padded_prefixes = einops.rearrange(\n",
    "        padded_prefixes, \"seq_len seqs batch_size -> (batch_size seqs) seq_len\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    padded_prefixes = torch.unique(padded_prefixes, dim=0)\n",
    "\n",
    "    all_prefixes.append(padded_prefixes)\n",
    "    \n",
    "    if batch_index % balance_interval == 0:\n",
    "        all_prefixes = balance_data(all_prefixes)\n",
    "        print(\n",
    "            f\"Batch {batch_index} - Data balanced. Size: {all_prefixes[0].size(0)}\")\n",
    "\n",
    "    batch_index += 1\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "all_prefixes = torch.cat(all_prefixes, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prefixes to a file\n",
    "prefixes_path = f\"data/{dataset_suffix}_{model_suffix}/prefixes.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(prefixes_path), exist_ok=True)\n",
    "\n",
    "torch.save(all_prefixes, prefixes_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
