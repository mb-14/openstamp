{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e533199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_dir_path = os.getenv(\"ROOT_PATH\")\n",
    "xe\n",
    "sys.path.append(f\"{root_dir_path}/os-watermarking\") #! add path for the watermarks\n",
    "sys.path.append(f\"{root_dir_path}/os-watermarking/MarkLLM\") #! add path for the watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import math\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from MarkLLM.watermark.auto_watermark import AutoWatermarkForVLLM\n",
    "from MarkLLM.utils.transformers_config import TransformersConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88faf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = os.getenv(\"MODEL\", \"meta-llama/Llama-2-7b-hf\")\n",
    "model_suffix = model_name.split(\"/\")[-1]\n",
    "output_file = os.getenv(\"OUTPUT_FILE\", None)\n",
    "\n",
    "assert output_file is not None, \"Please set the OUTPUT_FILE environment variable to the path of your output file.\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7de4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(output_file, \"r\") as f:\n",
    "    output_data = json.load(f)\n",
    "\n",
    "samples = output_data[\"samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a82be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "transformers_config = TransformersConfig(\n",
    "        model=AutoModelForCausalLM.from_pretrained(model_name),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "        vocab_size=config.vocab_size,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "watermark_type = output_data[\"watermark\"]\n",
    "config = output_data[\"config\"]\n",
    "\n",
    "#* Load the relevant watermark\n",
    "wtm_config = f'config/{watermark_type}/prefix_{config[\"prefix_length\"]}_gamma_{config[\"gamma\"]}_delta_{config[\"delta\"]}_key_{config[\"hash_key\"]}.json'\n",
    "watermark = AutoWatermarkForVLLM(algorithm_name=\"KGW\", \n",
    "                        algorithm_config=wtm_config, \n",
    "                        transformers_config=transformers_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50653aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zcores(column):\n",
    "    all_z_scores = []\n",
    "\n",
    "    data = samples[column]\n",
    "    for i in range(0, len(data)):\n",
    "        all_z_scores.append( watermark.detect_watermark(data[i])['score'] )\n",
    "    \n",
    "    all_z_scores = torch.tensor(all_z_scores)\n",
    "    # all_z_scores = torch.cat(all_z_scores)\n",
    "\n",
    "    return all_z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a23ca4",
   "metadata": {},
   "source": [
    "#### Human text / negative sample scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf860ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_z = get_zcores(\"human_text\")\n",
    "mean_negative_z = negative_z.mean().item()\n",
    "std_negative_z = negative_z.std().item()\n",
    "print(f\"Mean negative z value: {mean_negative_z}\")\n",
    "print(f\"Std negative z value: {std_negative_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b8589",
   "metadata": {},
   "source": [
    "#### Watermaked text score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(watermark_scores, null_scores):\n",
    "    min_sweep = min(watermark_scores.min().item(),\n",
    "                    null_scores.min().item()) - 1\n",
    "    max_sweep = max(watermark_scores.max().item(),\n",
    "                    null_scores.max().item()) + 1\n",
    "\n",
    "    # Compute AUROC\n",
    "    y_true = np.concatenate([\n",
    "        np.zeros_like(watermark_scores),\n",
    "        np.ones_like(null_scores)\n",
    "    ])\n",
    "    y_score = np.concatenate([watermark_scores, null_scores])\n",
    "\n",
    "    # Original logic: p-values â†’ low = positive = watermark\n",
    "    auroc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    # Compute best F1 score\n",
    "    f1_scores = []\n",
    "    thresholds = np.linspace(min_sweep, max_sweep, 1000)\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_score >= threshold).astype(int)  # Predict class 1 = null\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision +\n",
    "                                               recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "    best_f1_score = max(f1_scores)\n",
    "\n",
    "    # Interpolated TPR@1%FPR\n",
    "    fpr_array, tpr_array, _ = roc_curve(y_true, y_score)\n",
    "    tpr_interp = interp1d(fpr_array, tpr_array, kind='linear',\n",
    "                          bounds_error=False, fill_value=(tpr_array[0], tpr_array[-1]))\n",
    "    tpr_at_1_fpr = float(tpr_interp(0.01))\n",
    "    tpr_at_01_fpr = float(tpr_interp(0.001))\n",
    "\n",
    "    return {\n",
    "        \"auroc\": auroc,\n",
    "        \"best_f1_score\": best_f1_score,\n",
    "        \"tpr_1_fpr\": tpr_at_1_fpr,\n",
    "        \"tpr_0.1_fpr\": tpr_at_01_fpr,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_scores(column):\n",
    "    positive_z = get_zcores(\n",
    "        column)\n",
    "    mean_positive_z = positive_z.mean().item()\n",
    "    std_positive_z = positive_z.std().item()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(negative_z.cpu().numpy(), bins=50, alpha=0.5,\n",
    "             label='Negative Z-Scores', color='blue')\n",
    "    plt.hist(positive_z.cpu().numpy(), bins=50, alpha=0.5,\n",
    "             label='Positive Z-Scores', color='orange')\n",
    "    plt.title('Z-Score Distribution')\n",
    "    plt.xlabel('Z-Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean positive z value: {mean_positive_z}\")\n",
    "    print(f\"Std positive z value: {std_positive_z}\")\n",
    "    watermark_scores = positive_z.cpu().numpy()\n",
    "    null_scores = negative_z.cpu().numpy()\n",
    "    metrics = compute_metrics(-watermark_scores, -null_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_scores(\"model_text\")\n",
    "\n",
    "output_data[\"metrics\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde77335",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Clear the cache\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c83b7c",
   "metadata": {},
   "source": [
    "#### Paraphrased text score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165d9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = BATCH_SIZE // 2\n",
    "\n",
    "if \"dipper_text_lex60_order0\" in samples:\n",
    "    metrics = compute_scores(\"dipper_text_lex60_order0\")\n",
    "    output_data[\"metrics_dipper_text_lex60_order0\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"dipper_text_lex20_order0\" in samples:\n",
    "    metrics = compute_scores(\"dipper_text_lex20_order0\")\n",
    "    output_data[\"metrics_dipper_text_lex20_order0\"] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439a990",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f17b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output data to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
