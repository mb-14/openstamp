{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from mteb import MTEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miroojin/miniconda3/envs/venv/lib/python3.12/site-packages/mteb/evaluation/MTEB.py:121: UserWarning: Passing task names as strings is deprecated and will be removed in 2.0 release. Please use `tasks = mteb.get_tasks(tasks=[...])` method to get tasks instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sts_tasks = [\n",
    "    \"STSBenchmark\",   # Standard semantic similarity benchmark\n",
    "    # \"SICK-R\",         # Semantic relatedness from the SICK dataset\n",
    "    # \"STS12\",\n",
    "    # \"STS13\",\n",
    "    # \"STS14\",\n",
    "    # \"STS15\",\n",
    "    # \"STS16\",\n",
    "]\n",
    "\n",
    "evaluation = MTEB(tasks=sts_tasks)\n",
    "\n",
    "\n",
    "def eval(eval_model):\n",
    "    results = evaluation.run(eval_model, output_folder=None)\n",
    "    score = [result.scores['test'][0]['main_score']\n",
    "                     for result in results]\n",
    "    tasks = [result.task_name for result in results]\n",
    "    # Print list of tasks as comma-separated values\n",
    "    print(\", \".join(tasks))\n",
    "    # Print list of scores as comma-separated values    \n",
    "    print(\", \".join([f\"{score:.4f}\" for score in score]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = evaluation.tasks[0]\n",
    "\n",
    "task.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'split': ['test', 'test', 'test', 'test', 'test'],\n",
       " 'genre': ['main-captions',\n",
       "  'main-captions',\n",
       "  'main-captions',\n",
       "  'main-captions',\n",
       "  'main-captions'],\n",
       " 'dataset': ['MSRvid', 'MSRvid', 'MSRvid', 'MSRvid', 'MSRvid'],\n",
       " 'year': ['2012test', '2012test', '2012test', '2012test', '2012test'],\n",
       " 'sid': ['0024', '0033', '0045', '0063', '0066'],\n",
       " 'score': [2.5, 3.6, 5.0, 4.2, 1.5],\n",
       " 'sentence1': ['A girl is styling her hair.',\n",
       "  'A group of men play soccer on the beach.',\n",
       "  \"One woman is measuring another woman's ankle.\",\n",
       "  'A man is cutting up a cucumber.',\n",
       "  'A man is playing a harp.'],\n",
       " 'sentence2': ['A girl is brushing her hair.',\n",
       "  'A group of boys are playing soccer on the beach.',\n",
       "  \"A woman measures another woman's ankle.\",\n",
       "  'A man is slicing a cucumber.',\n",
       "  'A man is playing a keyboard.']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = task.dataset['test']\n",
    "\n",
    "test_dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LLaMAEmbeddingModel:\n",
    "    def __init__(self, model, tokenizer, normalize_embeddings=True, mean_pooling=True, layer_idx=-1, ignore_bos_token=False, batch_size=8, W_aug=torch.nn.Identity()):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.eval()\n",
    "        # Remove the second half of decoder layers\n",
    "        # self.model.model.layers = self.model.model.layers[:len(self.model.model.layers)//2]\n",
    "        self.model.lm_head = torch.nn.Identity()\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.mean_pooling = mean_pooling\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_idx = layer_idx\n",
    "        self.ignore_bos_token = ignore_bos_token\n",
    "        self.W_aug = W_aug.cuda()\n",
    "\n",
    "    def encode(self, sentences, **kwargs):\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(sentences), self.batch_size):\n",
    "            batch = sentences[i:i+self.batch_size]\n",
    "            # Update each sentence such that its templatized as \"Sentence: {sentence}, Repeat:\"\n",
    "            batch = [f\"Sentence: {sentence}\\n Meaning:\" for sentence in batch]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, return_tensors='pt', padding=True).to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model(\n",
    "                    **inputs, output_hidden_states=True, return_dict=True)\n",
    "                outputs = output.hidden_states[self.layer_idx]\n",
    "                # Mean pooling\n",
    "                if self.mean_pooling:\n",
    "                    attention_mask = inputs['attention_mask']\n",
    "                    if self.ignore_bos_token:\n",
    "                        # Find all positions of the BOS token\n",
    "                        bos_token_id = self.tokenizer.bos_token_id\n",
    "                        bos_positions = (inputs['input_ids'] == bos_token_id).nonzero(as_tuple=True)\n",
    "                        # Set the attention mask to 0 for BOS token positions, first dimension is batch size\n",
    "                        attention_mask[bos_positions] = 0\n",
    "\n",
    "                    embeddings = outputs * attention_mask.unsqueeze(-1)\n",
    "                    # Do weighted mean pooling based on position of token\n",
    "                    # basically weight = postion / sum(position)\n",
    "                    # Create position weights: 1-based indexing\n",
    "                    seq_length = attention_mask.size(1)\n",
    "                    position_ids = torch.arange(\n",
    "                       1, seq_length + 1, device=attention_mask.device).unsqueeze(0)  # (1, T)\n",
    "                    position_weights = position_ids * attention_mask  # (B, T)\n",
    "                    norm_factor = position_weights.sum(\n",
    "                        dim=1, keepdim=True).clamp(min=1e-5)  # avoid div-by-zero\n",
    "\n",
    "                    # Expand mask to match hidden dim\n",
    "                    # (B, T, 1)\n",
    "                    position_weights = position_weights.unsqueeze(-1)\n",
    "\n",
    "                    # Apply weighted pooling\n",
    "                    weighted_outputs = outputs * position_weights  # (B, T, D)\n",
    "                    embeddings = weighted_outputs.sum(\n",
    "                        dim=1) / norm_factor  # (B, D)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # get last token embedding\n",
    "                    embeddings = outputs[:, -1, :]\n",
    "                    # Apply ridge regression projection\n",
    "                    embeddings = self.W_aug(embeddings)\n",
    "\n",
    "                if self.normalize_embeddings:\n",
    "                    embeddings = torch.nn.functional.normalize(\n",
    "                        embeddings, p=2, dim=1)\n",
    "                all_embeddings.append(embeddings.cpu().float().numpy())\n",
    "        return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "eval_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "Correlation with oracle: 0.9382\n",
      "Correlation with scores (alpha=0.0): 0.0801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def template(\n",
    "    x): return f\"What does each word in this sentence mean: {x}\"\n",
    "\n",
    "\n",
    "def compute_entropy(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)  # (B, T)\n",
    "\n",
    "\n",
    "def compute_position_scores(mask):\n",
    "    B, T = mask.shape\n",
    "    position_ids = torch.arange(1, T + 1, device=mask.device).float()  # (T,)\n",
    "    position_scores = position_ids.unsqueeze(0).expand(B, -1) * mask  # (B, T)\n",
    "    norm_position = position_scores / \\\n",
    "        (position_scores.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    return norm_position\n",
    "\n",
    "\n",
    "def compute_normalized_entropy(entropy, mask):\n",
    "    inverted_entropy = 1.0 / (entropy + 1e-8)  # prevent division by zero\n",
    "    masked_entropy = inverted_entropy * mask\n",
    "    # Apply softmax with temperature 0.7\n",
    "    norm_entropy = F.softmax(masked_entropy, dim=1)  # (B, T)\n",
    "    return norm_entropy\n",
    "\n",
    "\n",
    "def compute_combined_weights(entropy, mask, alpha):\n",
    "    # Create low entropy mask\n",
    "    norm_entropy = compute_normalized_entropy(entropy, mask)\n",
    "    low_entropy_mask = (norm_entropy > 0.1).float() * mask  # (B, T)\n",
    "    # norm_entropy = compute_normalized_entropy(entropy, mask)\n",
    "    norm_position = compute_position_scores(low_entropy_mask)\n",
    "    return norm_position\n",
    "\n",
    "\n",
    "def compute_pooled_embedding(hidden_states, weights):\n",
    "    return torch.sum(hidden_states * weights.unsqueeze(-1), dim=1)  # (B, D)\n",
    "\n",
    "\n",
    "def process_sentences(model, tokenizer, sentences, alpha):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentences, return_tensors='pt',\n",
    "                           padding=True).to(model.device)\n",
    "        output = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "        logits = output.logits              # (B, T, V)\n",
    "        entropy = compute_entropy(logits)   # (B, T)\n",
    "\n",
    "        hidden = output.hidden_states[-3]   # (B, T, D3\n",
    "        mask = inputs['attention_mask']     # (B, T)\n",
    "\n",
    "        hidden = hidden[:, 1:, :]           # remove BOS → (B, T-1, D)\n",
    "        mask = mask[:, 1:]                  # (B, T-1)\n",
    "        entropy = entropy[:, 1:]           # (B, T-1)\n",
    "\n",
    "        weights = compute_combined_weights(entropy, mask, alpha)  # (B, T-1)\n",
    "        pooled = compute_pooled_embedding(hidden, weights)        # (B, D)\n",
    "\n",
    "        return pooled, inputs, weights\n",
    "\n",
    "\n",
    "def print_token_weights(inputs, weights, tokenizer, alpha):\n",
    "    input_ids = inputs['input_ids']\n",
    "    for i, (ids, w_row, mask_row) in enumerate(zip(input_ids, weights, inputs['attention_mask'][:, 1:])):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids[1:])  # skip BOS\n",
    "        print(f\"Sentence {i+1} tokens + weights (α = {alpha}):\")\n",
    "        for token, weight, valid in zip(tokens, w_row, mask_row):\n",
    "            if valid.item() == 1:\n",
    "                print(f\"  {token:>12} : weight = {weight.item():.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def oracle_embedding(sentences):\n",
    "    \"\"\"\n",
    "    This function computes the oracle embeddings for a list of sentences.\n",
    "    It uses the SentenceTransformer model to encode the sentences.\n",
    "    \"\"\"\n",
    "    return eval_model.encode(sentences, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "\n",
    "alphas = [0.0, 1.0, 0.7]\n",
    "\n",
    "index = 0\n",
    "print(test_dataset[index]['score'])\n",
    "# Get samples from evaluation\n",
    "sentence1 = test_dataset[index]['sentence1']\n",
    "sentence2 = test_dataset[index]['sentence2']\n",
    "\n",
    "\n",
    "n_samples = 200\n",
    "scores = [test_dataset[i]['score'] for i in range(n_samples)]\n",
    "sims2 = []\n",
    "oracle = []\n",
    "for i in range(n_samples):\n",
    "    sentence1 = test_dataset[i]['sentence1']\n",
    "    sentence2 = test_dataset[i]['sentence2']\n",
    "    \n",
    "    pooled_2 = process_sentences(\n",
    "        model, tokenizer, [sentence1, sentence2], alpha=1.0)[0]\n",
    "\n",
    "    sim2 = F.cosine_similarity(pooled_2[0].unsqueeze(\n",
    "        0), pooled_2[1].unsqueeze(0)).item()\n",
    "    oracle_emb = oracle_embedding([sentence1, sentence2])\n",
    "    oracle_sim = F.cosine_similarity(\n",
    "        oracle_emb[0].unsqueeze(0), oracle_emb[1].unsqueeze(0)).item()\n",
    "    oracle.append(oracle_sim)\n",
    "    sims2.append(sim2)\n",
    "# Print correlation with scores\n",
    "correlation2 = np.corrcoef(scores, sims2)[0, 1]\n",
    "correlation_oracle = np.corrcoef(scores, oracle)[0, 1]\n",
    "print(f\"Correlation with oracle: {correlation_oracle:.4f}\")\n",
    "print(f\"Correlation with scores (alpha=0.0): {correlation2:.4f}\")\n",
    "\n",
    "\n",
    "# n_samples = 200\n",
    "# print()\n",
    "# for alpha in alphas:\n",
    "#     sims = []\n",
    "#     for i in range(n_samples):\n",
    "#         sentence1 = test_dataset[i]['sentence1']\n",
    "#         sentence2 = test_dataset[i]['sentence2']\n",
    "#         pooled, inputs, weights = process_sentences(model, tokenizer, [sentence1, sentence2], alpha)\n",
    "#         sim = F.cosine_similarity(pooled[0].unsqueeze(0), pooled[1].unsqueeze(0)).item()\n",
    "#         sims.append(sim)\n",
    "\n",
    "#     # Print correlation with scores\n",
    "#     correlation = np.corrcoef(scores, sims)[0, 1]\n",
    "#     print(f\"Alpha: {alpha}, Correlation with scores: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 tokens + weights (α = 1.0):\n",
      "             A : weight = 0.0357\n",
      "         Ġgirl : weight = 0.0714\n",
      "           Ġis : weight = 0.1071\n",
      "      Ġstyling : weight = 0.1429\n",
      "          Ġher : weight = 0.1786\n",
      "         Ġhair : weight = 0.2143\n",
      "             . : weight = 0.2500\n",
      "\n",
      "Sentence 2 tokens + weights (α = 1.0):\n",
      "             A : weight = 0.0357\n",
      "         Ġgirl : weight = 0.0714\n",
      "           Ġis : weight = 0.1071\n",
      "     Ġbrushing : weight = 0.1429\n",
      "          Ġher : weight = 0.1786\n",
      "         Ġhair : weight = 0.2143\n",
      "             . : weight = 0.2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1 =test_dataset[0]['sentence1']\n",
    "s2 = test_dataset[0]['sentence2']\n",
    "pooled, inputs, weights = process_sentences(model, tokenizer, [s1, s2], alpha=1.0)\n",
    "print_token_weights(inputs, weights, tokenizer, alpha=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, mean_pooling=False)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models.overview:Failed to extract metadata from model: 'LLaMAEmbeddingModel' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">STS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSTS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STSBenchmark, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STSBenchmark, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STSBenchmark\n",
      "0.5314\n",
      "[0.5314290129100447]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, ignore_bos_token=True, mean_pooling=True, batch_size=16)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:mteb.models.overview:Failed to extract metadata from model: 'LLaMAEmbeddingModel' object has no attribute 'model_card_data'. Upgrading to sentence-transformers v3.0.0 or above is recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">STS</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSTS\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - STSBenchmark, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2s</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - STSBenchmark, \u001b[3;38;5;241ms2s\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STSBenchmark\n",
      "0.0893\n",
      "[0.08927839385848074]\n"
     ]
    }
   ],
   "source": [
    "n_layers = len(model.model.layers)\n",
    "\n",
    "eval_model = LLaMAEmbeddingModel(\n",
    "    model, tokenizer, normalize_embeddings=False, mean_pooling=False, ignore_bos_token=False, layer_idx=-4)\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "print(eval(eval_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"WhereIsAI/UAE-Large-V1\")\n",
    "print(eval(model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
